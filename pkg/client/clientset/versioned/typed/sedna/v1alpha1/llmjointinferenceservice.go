/*
Copyright The KubeEdge Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by client-gen. DO NOT EDIT.

package v1alpha1

import (
	"context"
	"time"

	v1alpha1 "github.com/kubeedge/sedna/pkg/apis/sedna/v1alpha1"
	scheme "github.com/kubeedge/sedna/pkg/client/clientset/versioned/scheme"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	types "k8s.io/apimachinery/pkg/types"
	watch "k8s.io/apimachinery/pkg/watch"
	rest "k8s.io/client-go/rest"
)

// LLMJointInferenceServicesGetter has a method to return a LLMJointInferenceServiceInterface.
// A group's client should implement this interface.
type LLMJointInferenceServicesGetter interface {
	LLMJointInferenceServices(namespace string) LLMJointInferenceServiceInterface
}

// LLMJointInferenceServiceInterface has methods to work with LLMJointInferenceService resources.
type LLMJointInferenceServiceInterface interface {
	Create(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.CreateOptions) (*v1alpha1.LLMJointInferenceService, error)
	Update(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.UpdateOptions) (*v1alpha1.LLMJointInferenceService, error)
	UpdateStatus(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.UpdateOptions) (*v1alpha1.LLMJointInferenceService, error)
	Delete(ctx context.Context, name string, opts v1.DeleteOptions) error
	DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error
	Get(ctx context.Context, name string, opts v1.GetOptions) (*v1alpha1.LLMJointInferenceService, error)
	List(ctx context.Context, opts v1.ListOptions) (*v1alpha1.LLMJointInferenceServiceList, error)
	Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error)
	Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *v1alpha1.LLMJointInferenceService, err error)
	LLMJointInferenceServiceExpansion
}

// lLMJointInferenceServices implements LLMJointInferenceServiceInterface
type lLMJointInferenceServices struct {
	client rest.Interface
	ns     string
}

// newLLMJointInferenceServices returns a LLMJointInferenceServices
func newLLMJointInferenceServices(c *SednaV1alpha1Client, namespace string) *lLMJointInferenceServices {
	return &lLMJointInferenceServices{
		client: c.RESTClient(),
		ns:     namespace,
	}
}

// Get takes name of the lLMJointInferenceService, and returns the corresponding lLMJointInferenceService object, and an error if there is any.
func (c *lLMJointInferenceServices) Get(ctx context.Context, name string, options v1.GetOptions) (result *v1alpha1.LLMJointInferenceService, err error) {
	result = &v1alpha1.LLMJointInferenceService{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		Name(name).
		VersionedParams(&options, scheme.ParameterCodec).
		Do(ctx).
		Into(result)
	return
}

// List takes label and field selectors, and returns the list of LLMJointInferenceServices that match those selectors.
func (c *lLMJointInferenceServices) List(ctx context.Context, opts v1.ListOptions) (result *v1alpha1.LLMJointInferenceServiceList, err error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	result = &v1alpha1.LLMJointInferenceServiceList{}
	err = c.client.Get().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Do(ctx).
		Into(result)
	return
}

// Watch returns a watch.Interface that watches the requested lLMJointInferenceServices.
func (c *lLMJointInferenceServices) Watch(ctx context.Context, opts v1.ListOptions) (watch.Interface, error) {
	var timeout time.Duration
	if opts.TimeoutSeconds != nil {
		timeout = time.Duration(*opts.TimeoutSeconds) * time.Second
	}
	opts.Watch = true
	return c.client.Get().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		VersionedParams(&opts, scheme.ParameterCodec).
		Timeout(timeout).
		Watch(ctx)
}

// Create takes the representation of a lLMJointInferenceService and creates it.  Returns the server's representation of the lLMJointInferenceService, and an error, if there is any.
func (c *lLMJointInferenceServices) Create(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.CreateOptions) (result *v1alpha1.LLMJointInferenceService, err error) {
	result = &v1alpha1.LLMJointInferenceService{}
	err = c.client.Post().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(lLMJointInferenceService).
		Do(ctx).
		Into(result)
	return
}

// Update takes the representation of a lLMJointInferenceService and updates it. Returns the server's representation of the lLMJointInferenceService, and an error, if there is any.
func (c *lLMJointInferenceServices) Update(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.UpdateOptions) (result *v1alpha1.LLMJointInferenceService, err error) {
	result = &v1alpha1.LLMJointInferenceService{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		Name(lLMJointInferenceService.Name).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(lLMJointInferenceService).
		Do(ctx).
		Into(result)
	return
}

// UpdateStatus was generated because the type contains a Status member.
// Add a +genclient:noStatus comment above the type to avoid generating UpdateStatus().
func (c *lLMJointInferenceServices) UpdateStatus(ctx context.Context, lLMJointInferenceService *v1alpha1.LLMJointInferenceService, opts v1.UpdateOptions) (result *v1alpha1.LLMJointInferenceService, err error) {
	result = &v1alpha1.LLMJointInferenceService{}
	err = c.client.Put().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		Name(lLMJointInferenceService.Name).
		SubResource("status").
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(lLMJointInferenceService).
		Do(ctx).
		Into(result)
	return
}

// Delete takes name of the lLMJointInferenceService and deletes it. Returns an error if one occurs.
func (c *lLMJointInferenceServices) Delete(ctx context.Context, name string, opts v1.DeleteOptions) error {
	return c.client.Delete().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		Name(name).
		Body(&opts).
		Do(ctx).
		Error()
}

// DeleteCollection deletes a collection of objects.
func (c *lLMJointInferenceServices) DeleteCollection(ctx context.Context, opts v1.DeleteOptions, listOpts v1.ListOptions) error {
	var timeout time.Duration
	if listOpts.TimeoutSeconds != nil {
		timeout = time.Duration(*listOpts.TimeoutSeconds) * time.Second
	}
	return c.client.Delete().
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		VersionedParams(&listOpts, scheme.ParameterCodec).
		Timeout(timeout).
		Body(&opts).
		Do(ctx).
		Error()
}

// Patch applies the patch and returns the patched lLMJointInferenceService.
func (c *lLMJointInferenceServices) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts v1.PatchOptions, subresources ...string) (result *v1alpha1.LLMJointInferenceService, err error) {
	result = &v1alpha1.LLMJointInferenceService{}
	err = c.client.Patch(pt).
		Namespace(c.ns).
		Resource("llmjointinferenceservices").
		Name(name).
		SubResource(subresources...).
		VersionedParams(&opts, scheme.ParameterCodec).
		Body(data).
		Do(ctx).
		Into(result)
	return
}
